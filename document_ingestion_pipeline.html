<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>RNA Lab Navigator - Document Ingestion Pipeline</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      line-height: 1.6;
      color: #333;
      max-width: 1200px;
      margin: 0 auto;
      padding: 20px;
    }
    
    h1, h2, h3, h4 {
      color: #2c3e50;
    }
    
    .container {
      margin-bottom: 30px;
    }
    
    .pipeline-diagram {
      background: linear-gradient(135deg, #f6f9fc 0%, #eef6fd 100%);
      border-radius: 10px;
      padding: 30px;
      margin-bottom: 30px;
      box-shadow: 0 3px 10px rgba(0, 0, 0, 0.08);
    }
    
    .flow-diagram {
      display: flex;
      flex-direction: column;
      align-items: center;
      margin: 30px 0;
    }
    
    .flow-row {
      display: flex;
      justify-content: center;
      width: 100%;
      margin-bottom: 15px;
    }
    
    .flow-box {
      border-radius: 8px;
      padding: 15px;
      margin: 0 15px;
      color: white;
      box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);
      display: flex;
      flex-direction: column;
      justify-content: center;
      min-height: 80px;
      text-align: center;
      width: 180px;
    }
    
    .flow-title {
      font-weight: bold;
      font-size: 16px;
      margin-bottom: 5px;
    }
    
    .flow-description {
      font-size: 12px;
    }
    
    .flow-arrow {
      display: flex;
      align-items: center;
      color: #7f8c8d;
      font-size: 24px;
    }
    
    .flow-arrow.down {
      flex-direction: column;
      height: 50px;
    }
    
    .blue-gradient {
      background: linear-gradient(145deg, #3498db 0%, #2980b9 100%);
    }
    
    .red-gradient {
      background: linear-gradient(145deg, #e74c3c 0%, #c0392b 100%);
    }
    
    .purple-gradient {
      background: linear-gradient(145deg, #9b59b6 0%, #8e44ad 100%);
    }
    
    .green-gradient {
      background: linear-gradient(145deg, #2ecc71 0%, #27ae60 100%);
    }
    
    .orange-gradient {
      background: linear-gradient(145deg, #f39c12 0%, #e67e22 100%);
    }
    
    .teal-gradient {
      background: linear-gradient(145deg, #16a085 0%, #1abc9c 100%);
    }
    
    .yellow-gradient {
      background: linear-gradient(145deg, #f1c40f 0%, #f39c12 100%);
    }
    
    .code-block {
      background-color: #f8f9fa;
      border-radius: 6px;
      padding: 15px;
      margin: 15px 0;
      overflow-x: auto;
      font-family: monospace;
      border-left: 4px solid #3498db;
    }
    
    .pipeline-detail {
      margin-top: 20px;
      background-color: white;
      border-radius: 8px;
      padding: 20px;
      box-shadow: 0 2px 4px rgba(0,0,0,0.05);
    }
    
    .source-type {
      display: flex;
      background-color: #f8f9fa;
      border-radius: 8px;
      margin-bottom: 20px;
      overflow: hidden;
      box-shadow: 0 2px 5px rgba(0,0,0,0.05);
    }
    
    .source-title {
      background-color: #2c3e50;
      color: white;
      padding: 15px;
      min-width: 150px;
      display: flex;
      align-items: center;
      justify-content: center;
      font-weight: bold;
    }
    
    .source-content {
      padding: 15px;
      flex: 1;
    }
    
    .source-content ul {
      margin: 0;
      padding-left: 20px;
    }
    
    .metrics {
      display: flex;
      justify-content: space-around;
      flex-wrap: wrap;
      margin-top: 20px;
      margin-bottom: 20px;
    }
    
    .metric-box {
      background-color: white;
      border-radius: 8px;
      padding: 15px;
      margin: 10px;
      min-width: 150px;
      box-shadow: 0 3px 6px rgba(0,0,0,0.1);
      text-align: center;
    }
    
    .metric-title {
      font-weight: bold;
      margin-bottom: 8px;
      color: #2c3e50;
    }
    
    .metric-value {
      font-size: 24px;
      color: #2980b9;
    }
    
    .two-column {
      display: flex;
      flex-wrap: wrap;
      gap: 20px;
      margin: 20px 0;
    }
    
    .column {
      flex: 1;
      min-width: 300px;
    }
    
    /* Responsive adjustments */
    @media (max-width: 768px) {
      .flow-row {
        flex-direction: column;
        align-items: center;
      }
      
      .flow-box {
        margin-bottom: 20px;
        width: 80%;
      }
      
      .flow-arrow.horizontal {
        transform: rotate(90deg);
        margin: 10px 0;
      }
      
      .source-type {
        flex-direction: column;
      }
      
      .source-title {
        min-width: auto;
        width: 100%;
      }
      
      .two-column {
        flex-direction: column;
      }
    }
  </style>
</head>
<body>
  <h1>RNA Lab Navigator: Document Ingestion Pipeline</h1>
  
  <div class="container">
    <p>
      The document ingestion pipeline is a critical component of the RNA Lab Navigator, responsible for processing 
      various types of documents (protocols, theses, papers) and preparing them for efficient retrieval. This 
      document provides a detailed technical overview of the ingestion process, focusing on the specialized 
      handling of different document types and optimization strategies.
    </p>
  </div>
  
  <!-- Document Ingestion Flow Diagram -->
  <div class="pipeline-diagram">
    <h2 style="text-align: center; margin-bottom: 30px;">Document Ingestion Workflow</h2>
    
    <div class="flow-diagram">
      <div class="flow-row">
        <div class="flow-box blue-gradient">
          <div class="flow-title">Input Sources</div>
          <div class="flow-description">Protocols, Theses, Papers, BioRxiv</div>
        </div>
      </div>
      
      <div class="flow-arrow down">↓</div>
      
      <div class="flow-row">
        <div class="flow-box red-gradient">
          <div class="flow-title">Text Extraction</div>
          <div class="flow-description">PyMuPDF, pdfplumber</div>
        </div>
      </div>
      
      <div class="flow-arrow down">↓</div>
      
      <div class="flow-row">
        <div class="flow-box purple-gradient">
          <div class="flow-title">Document Chunking</div>
          <div class="flow-description">400±50 words, 100-word overlap</div>
        </div>
      </div>
      
      <div class="flow-arrow down">↓</div>
      
      <div class="flow-row">
        <div class="flow-box orange-gradient">
          <div class="flow-title">Metadata Extraction</div>
          <div class="flow-description">Source, Author, Date, Type</div>
        </div>
        <div class="flow-arrow horizontal">→</div>
        <div class="flow-box yellow-gradient">
          <div class="flow-title">Citation Generation</div>
          <div class="flow-description">Source IDs, Page Numbers</div>
        </div>
      </div>
      
      <div class="flow-arrow down">↓</div>
      
      <div class="flow-row">
        <div class="flow-box teal-gradient">
          <div class="flow-title">Embedding Generation</div>
          <div class="flow-description">Ada-002 (1536-dim)</div>
        </div>
      </div>
      
      <div class="flow-arrow down">↓</div>
      
      <div class="flow-row">
        <div class="flow-box green-gradient">
          <div class="flow-title">Weaviate Storage</div>
          <div class="flow-description">HNSW + BM25 Hybrid Index</div>
        </div>
      </div>
    </div>
  </div>
  
  <!-- Document Source Types -->
  <div class="container">
    <h2>Document Source Handling</h2>
    <p>The ingestion pipeline employs specialized handling for different document types:</p>
    
    <div class="source-type">
      <div class="source-title blue-gradient">
        Lab Protocols
      </div>
      <div class="source-content">
        <ul>
          <li><strong>Extraction Method:</strong> pdfplumber for complex lab document layouts</li>
          <li><strong>Chunking Strategy:</strong> Standard 400±50 words with reagent section detection</li>
          <li><strong>Metadata:</strong> Protocol ID, category, last modified date, author, reagents list</li>
          <li><strong>Special Handling:</strong> Table extraction for reagent concentrations and steps</li>
          <li><strong>Version Tracking:</strong> Maintains protocol versioning history</li>
        </ul>
      </div>
    </div>
    
    <div class="source-type">
      <div class="source-title red-gradient">
        PhD Theses
      </div>
      <div class="source-content">
        <ul>
          <li><strong>Extraction Method:</strong> PyMuPDF with OCR fallback for scanned sections</li>
          <li><strong>Chunking Strategy:</strong> Chapter-based primary split, then 400±50 word chunks</li>
          <li><strong>Metadata:</strong> Author, year, department, chapter titles, page numbers</li>
          <li><strong>Special Handling:</strong> Figure extraction and reference linking</li>
          <li><strong>Structure Preservation:</strong> Chapter/section hierarchy maintained in metadata</li>
        </ul>
      </div>
    </div>
    
    <div class="source-type">
      <div class="source-title purple-gradient">
        Research Papers
      </div>
      <div class="source-content">
        <ul>
          <li><strong>Extraction Method:</strong> PyPDF2 with column detection for academic layouts</li>
          <li><strong>Chunking Strategy:</strong> Section-aware chunking (abstract, methods, results, discussion)</li>
          <li><strong>Metadata:</strong> Authors, journal, publication date, DOI, keywords</li>
          <li><strong>Special Handling:</strong> Table and figure extraction with caption linking</li>
          <li><strong>Citation Linking:</strong> Cross-reference resolution for cited works</li>
        </ul>
      </div>
    </div>
    
    <div class="source-type">
      <div class="source-title orange-gradient">
        BioRxiv Preprints
      </div>
      <div class="source-content">
        <ul>
          <li><strong>Extraction Method:</strong> API-based fetching via Celery Beat (daily schedule)</li>
          <li><strong>Filtering:</strong> RNA biology keyword matching (configurable)</li>
          <li><strong>Content:</strong> Abstract + title as initial content, with full-text on demand</li>
          <li><strong>Metadata:</strong> Authors, submission date, category, DOI, version</li>
          <li><strong>Update Strategy:</strong> Incremental update for revised preprints</li>
        </ul>
      </div>
    </div>
  </div>
  
  <!-- Technical Implementation Details -->
  <div class="pipeline-diagram">
    <h2 style="text-align: center; margin-bottom: 20px;">Technical Implementation</h2>
    
    <div class="pipeline-detail">
      <h3>1. Text Extraction</h3>
      
      <div class="two-column">
        <div class="column">
          <p>The extraction module uses a multi-tool approach with fallback mechanisms:</p>
          <ul>
            <li><strong>Primary Tools:</strong> PyMuPDF, pdfplumber, PyPDF2</li>
            <li><strong>OCR Integration:</strong> Tesseract for scanned documents</li>
            <li><strong>Layout Analysis:</strong> Page segmentation for columns and regions</li>
            <li><strong>Error Handling:</strong> Graceful fallback for corrupted pages</li>
          </ul>
          
          <p>The tool selection is automatic based on document analysis:</p>
          <ul>
            <li>pdfplumber: Complex layouts, tables, forms</li>
            <li>PyMuPDF: General-purpose extraction with good performance</li>
            <li>Tesseract OCR: When digital text extraction fails</li>
          </ul>
        </div>
        
        <div class="column">
          <div class="code-block">
            <pre>def extract_text(pdf_path):
    """
    Extract text from PDF using appropriate tools
    """
    # Analyze PDF to determine best extraction method
    extraction_method = analyze_pdf_structure(pdf_path)
    
    if extraction_method == "pdfplumber":
        return extract_with_pdfplumber(pdf_path)
    elif extraction_method == "pymupdf":
        return extract_with_pymupdf(pdf_path)
    elif extraction_method == "ocr":
        return extract_with_ocr(pdf_path)
    else:
        # Fallback to combined approach
        return extract_combined(pdf_path)

def analyze_pdf_structure(pdf_path):
    """
    Determine the appropriate extraction method
    based on PDF structure analysis
    """
    with fitz.open(pdf_path) as doc:
        # Check if document has selectable text
        has_text = False
        for page in doc:
            if page.get_text():
                has_text = True
                break
        
        # If no selectable text, use OCR
        if not has_text:
            return "ocr"
        
        # Check for complex layouts (tables, multi-column)
        has_complex_layout = False
        sample_page = doc[0]
        blocks = sample_page.get_text("blocks")
        if len(blocks) > 10:  # Heuristic for complex layout
            has_complex_layout = True
        
        # Return appropriate method
        if has_complex_layout:
            return "pdfplumber"
        else:
            return "pymupdf"</pre>
          </div>
        </div>
      </div>
    </div>
    
    <div class="pipeline-detail">
      <h3>2. Intelligent Chunking</h3>
      
      <div class="two-column">
        <div class="column">
          <p>The chunking system balances several factors:</p>
          <ul>
            <li><strong>Size:</strong> Target 400±50 words per chunk</li>
            <li><strong>Overlap:</strong> 100 words between chunks to maintain context</li>
            <li><strong>Semantic Boundaries:</strong> Preference for section/paragraph breaks</li>
            <li><strong>Special Case - Theses:</strong> Chapter detection with regex patterns</li>
            <li><strong>Special Case - Papers:</strong> Section awareness (Methods, Results, etc.)</li>
          </ul>
          
          <p>Key features include:</p>
          <ul>
            <li>Preservation of hierarchical information (chapter → section → paragraph)</li>
            <li>Table and figure handling to avoid mid-content breaks</li>
            <li>Reference preservation to maintain citation context</li>
            <li>Reagent list completeness in protocol chunks</li>
          </ul>
        </div>
        
        <div class="column">
          <div class="code-block">
            <pre>def chunk_document(text, doc_type, metadata=None):
    """
    Intelligently chunk document based on type and content
    """
    if doc_type == "thesis":
        # For theses, first split by chapters
        chapter_pattern = r'CHAPTER\s+\d+[\s\-:]*([^\n]+)'
        chapters = re.split(chapter_pattern, text)
        chunks = []
        
        for i, chapter in enumerate(chapters):
            if i % 2 == 1:  # Chapter titles are at odd indices
                chapter_title = chapter.strip()
                chapter_content = chapters[i+1] if i+1 < len(chapters) else ""
                
                # For each chapter, create word chunks
                chapter_chunks = create_word_chunks(
                    chapter_content, 
                    chunk_size=400, 
                    overlap=100
                )
                
                # Add chapter metadata to each chunk
                for j, chunk in enumerate(chapter_chunks):
                    chunk_metadata = metadata.copy() if metadata else {}
                    chunk_metadata.update({
                        "chapter": chapter_title,
                        "chunk_index": j,
                        "chapter_index": i // 2
                    })
                    chunks.append({
                        "content": chunk,
                        "metadata": chunk_metadata
                    })
        
        return chunks
    
    elif doc_type == "protocol":
        # For protocols, preserve reagent sections
        sections = split_by_protocol_sections(text)
        chunks = []
        
        for section_name, section_content in sections:
            # Create word chunks within each section
            section_chunks = create_word_chunks(
                section_content,
                chunk_size=400,
                overlap=100
            )
            
            # Add section metadata to each chunk
            for j, chunk in enumerate(section_chunks):
                chunk_metadata = metadata.copy() if metadata else {}
                chunk_metadata.update({
                    "section": section_name,
                    "chunk_index": j
                })
                chunks.append({
                    "content": chunk,
                    "metadata": chunk_metadata
                })
        
        return chunks
    
    else:  # Default for papers and other types
        standard_chunks = create_word_chunks(
            text,
            chunk_size=400,
            overlap=100
        )
        
        return [
            {
                "content": chunk,
                "metadata": {
                    **(metadata or {}),
                    "chunk_index": i
                }
            }
            for i, chunk in enumerate(standard_chunks)
        ]</pre>
          </div>
        </div>
      </div>
    </div>
    
    <div class="pipeline-detail">
      <h3>3. Metadata Extraction</h3>
      
      <p>The metadata extraction module employs a combination of rule-based and ML approaches:</p>
      
      <div class="two-column">
        <div class="column">
          <h4>Core Metadata Fields</h4>
          <ul>
            <li><strong>Common Fields:</strong>
              <ul>
                <li>doc_type: "thesis", "protocol", "paper", "preprint"</li>
                <li>source_file: Original filename/path</li>
                <li>ingestion_date: ISO timestamp of processing</li>
                <li>word_count: Total words in chunk</li>
                <li>chunk_index: Position in sequence</li>
              </ul>
            </li>
            <li><strong>Type-Specific Fields:</strong>
              <ul>
                <li>Theses: author, year, department, institution, advisor</li>
                <li>Protocols: author, version, creation_date, update_date, category</li>
                <li>Papers: authors, journal, publication_date, doi, keywords</li>
              </ul>
            </li>
          </ul>
        </div>
        
        <div class="column">
          <h4>Extraction Techniques</h4>
          <ul>
            <li><strong>Rule-based Extraction:</strong>
              <ul>
                <li>Regular expressions for structured fields</li>
                <li>Pattern matching for dates, DOIs, author names</li>
                <li>Title page parsing for thesis metadata</li>
              </ul>
            </li>
            <li><strong>ML-based Extraction:</strong>
              <ul>
                <li>Named Entity Recognition for authors and institutions</li>
                <li>Document classification for protocol categories</li>
                <li>Keyword extraction for paper topics</li>
              </ul>
            </li>
            <li><strong>Manual Override:</strong>
              <ul>
                <li>Web UI for metadata verification and correction</li>
                <li>Batch update functionality for curators</li>
              </ul>
            </li>
          </ul>
        </div>
      </div>
    </div>
    
    <div class="pipeline-detail">
      <h3>4. Embedding Generation</h3>
      
      <div class="two-column">
        <div class="column">
          <p>The embedding generation system prioritizes quality and efficiency:</p>
          <ul>
            <li><strong>Primary Model:</strong> OpenAI text-embedding-ada-002 (1536 dimensions)</li>
            <li><strong>Fallback Model:</strong> SentenceTransformers all-MiniLM-L6-v2 (384 dimensions)</li>
            <li><strong>Batch Processing:</strong> 20 chunks per API call to minimize overhead</li>
            <li><strong>Caching Strategy:</strong> SHA-256 hash-based with 30-day TTL</li>
          </ul>
          
          <p>Optimization techniques include:</p>
          <ul>
            <li>Intelligent batching to maximize API efficiency</li>
            <li>Concurrent processing for large document sets</li>
            <li>Redis-based caching to prevent redundant embedding generation</li>
            <li>Rate limiting and retry logic for API stability</li>
          </ul>
        </div>
        
        <div class="column">
          <div class="metrics">
            <div class="metric-box">
              <div class="metric-title">Embedding Time</div>
              <div class="metric-value">0.5s</div>
              <div>per chunk (avg)</div>
            </div>
            
            <div class="metric-box">
              <div class="metric-title">Cache Hit Rate</div>
              <div class="metric-value">42%</div>
              <div>for new documents</div>
            </div>
            
            <div class="metric-box">
              <div class="metric-title">API Cost</div>
              <div class="metric-value">$0.06</div>
              <div>per MB of text</div>
            </div>
          </div>
          
          <div class="code-block">
            <pre>def generate_embedding_batch(chunks, use_cache=True):
    """
    Generate embeddings for a batch of chunks with caching
    """
    # Check cache for existing embeddings
    cache_hits = []
    chunks_to_embed = []
    chunk_hashes = []
    
    for chunk in chunks:
        chunk_text = chunk["content"]
        chunk_hash = hashlib.sha256(chunk_text.encode()).hexdigest()
        chunk_hashes.append(chunk_hash)
        
        if use_cache and redis_client.exists(f"emb:{chunk_hash}"):
            # Retrieve from cache
            embedding = json.loads(redis_client.get(f"emb:{chunk_hash}"))
            cache_hits.append({
                "chunk": chunk,
                "embedding": embedding,
                "source": "cache"
            })
        else:
            chunks_to_embed.append(chunk)
    
    # If all chunks were in cache, return early
    if not chunks_to_embed:
        return cache_hits
    
    # Generate embeddings for remaining chunks
    try:
        response = openai.Embedding.create(
            input=[c["content"] for c in chunks_to_embed],
            model="text-embedding-ada-002"
        )
        
        # Process response and update cache
        api_results = []
        for i, chunk in enumerate(chunks_to_embed):
            embedding = response["data"][i]["embedding"]
            chunk_hash = chunk_hashes[len(cache_hits) + i]
            
            # Cache the embedding
            if use_cache:
                redis_client.setex(
                    f"emb:{chunk_hash}",
                    60 * 60 * 24 * 30,  # 30 day TTL
                    json.dumps(embedding)
                )
            
            api_results.append({
                "chunk": chunk,
                "embedding": embedding,
                "source": "api"
            })
        
        # Combine cache hits and API results
        return cache_hits + api_results
        
    except Exception as e:
        # Fallback to local model on API failure
        logger.warning(f"OpenAI API failed, falling back to local model: {e}")
        
        local_model = SentenceTransformer("all-MiniLM-L6-v2")
        embeddings = local_model.encode(
            [c["content"] for c in chunks_to_embed],
            batch_size=32,
            show_progress_bar=False
        )
        
        fallback_results = []
        for i, chunk in enumerate(chunks_to_embed):
            embedding = embeddings[i].tolist()
            fallback_results.append({
                "chunk": chunk,
                "embedding": embedding,
                "source": "local_model"
            })
        
        return cache_hits + fallback_results</pre>
          </div>
        </div>
      </div>
    </div>
    
    <div class="pipeline-detail">
      <h3>5. Vector Database Storage</h3>
      
      <div class="two-column">
        <div class="column">
          <p>The Weaviate configuration is optimized for RNA biology domain:</p>
          <ul>
            <li><strong>Index Type:</strong> HNSW (Hierarchical Navigable Small World)</li>
            <li><strong>Distance Metric:</strong> Cosine similarity</li>
            <li><strong>HNSW Parameters:</strong>
              <ul>
                <li>ef: 256 (search complexity)</li>
                <li>efConstruction: 256 (index build complexity)</li>
                <li>maxConnections: 64 (nodes per level)</li>
              </ul>
            </li>
            <li><strong>Hybrid Search:</strong> BM25 enabled with vector:keyword ratio of 0.75:0.25</li>
          </ul>
          
          <p>Schema design includes:</p>
          <ul>
            <li>Document class with indexFilterable properties</li>
            <li>Text analyzers for biomedical terminology</li>
            <li>Cross-references between related chunks</li>
            <li>Custom tokenization for RNA biology terms</li>
          </ul>
        </div>
        
        <div class="column">
          <div class="code-block">
            <pre>class_obj = {
    "class": "Document",
    "vectorizer": "none",  # We provide vectors directly
    "vectorIndexConfig": {
        "distance": "cosine",
        "ef": 256,
        "efConstruction": 256,
        "maxConnections": 64,
    },
    "moduleConfig": {
        "text2vec-contextionary": {
            "skip": True  # Skip built-in vectorization
        },
        "text2vec-transformers": {
            "skip": True  # Skip built-in vectorization
        }
    },
    "properties": [
        {"name": "content", "dataType": ["text"]},
        {"name": "doc_type", "dataType": ["text"], "indexFilterable": True},
        {"name": "source_file", "dataType": ["text"], "indexFilterable": True},
        {"name": "chunk_index", "dataType": ["int"]},
        {"name": "word_count", "dataType": ["int"]},
        {"name": "ingestion_date", "dataType": ["date"]},
        
        # Thesis-specific properties
        {"name": "author", "dataType": ["text"], "indexFilterable": True},
        {"name": "year", "dataType": ["int"], "indexFilterable": True},
        {"name": "department", "dataType": ["text"], "indexFilterable": True},
        {"name": "institution", "dataType": ["text"], "indexFilterable": True},
        {"name": "advisor", "dataType": ["text"]},
        {"name": "chapter", "dataType": ["text"], "indexFilterable": True},
        
        # Protocol-specific properties
        {"name": "version", "dataType": ["string"]},
        {"name": "creation_date", "dataType": ["date"]},
        {"name": "update_date", "dataType": ["date"]},
        {"name": "category", "dataType": ["text"], "indexFilterable": True},
        {"name": "reagents", "dataType": ["text[]"]},
        
        # Paper-specific properties
        {"name": "authors", "dataType": ["text[]"]},
        {"name": "journal", "dataType": ["text"], "indexFilterable": True},
        {"name": "publication_date", "dataType": ["date"]},
        {"name": "doi", "dataType": ["text"]},
        {"name": "keywords", "dataType": ["text[]"], "indexFilterable": True},
        
        # Cross-references
        {"name": "references", "dataType": ["Document[]"]}
    ]
}</pre>
          </div>
        </div>
      </div>
    </div>
  </div>
  
  <!-- Performance Metrics -->
  <div class="container">
    <h2>Pipeline Performance Metrics</h2>
    
    <div class="metrics">
      <div class="metric-box">
        <div class="metric-title">Processing Speed</div>
        <div class="metric-value">1.2 MB/min</div>
        <div>End-to-end pipeline</div>
      </div>
      
      <div class="metric-box">
        <div class="metric-title">Average Chunks</div>
        <div class="metric-value">120</div>
        <div>Per PhD thesis</div>
      </div>
      
      <div class="metric-box">
        <div class="metric-title">Average Chunks</div>
        <div class="metric-value">15</div>
        <div>Per protocol document</div>
      </div>
      
      <div class="metric-box">
        <div class="metric-title">Average Chunks</div>
        <div class="metric-value">25</div>
        <div>Per research paper</div>
      </div>
      
      <div class="metric-box">
        <div class="metric-title">Memory Usage</div>
        <div class="metric-value">2-4 GB</div>
        <div>Peak processing RAM</div>
      </div>
    </div>
    
    <p style="text-align: center;">
      Storage requirements: ~2 KB per chunk metadata + ~12 KB per vector embedding = ~14 KB per chunk
    </p>
  </div>
  
  <!-- Ongoing Improvements -->
  <div class="pipeline-diagram">
    <h2 style="text-align: center; margin-bottom: 20px;">In-Progress Enhancements</h2>
    
    <div class="two-column">
      <div class="column">
        <div class="pipeline-detail">
          <h3>Figure & Image Extraction</h3>
          <p>Automated extraction and embedding of figures from documents:</p>
          <ul>
            <li>PDF page image extraction with context</li>
            <li>CLIP-based embeddings for image-text similarity</li>
            <li>Figure caption parsing and linking</li>
            <li>Multimodal search capabilities</li>
          </ul>
          <p>This enhancement will enable the system to include relevant figures in responses.</p>
        </div>
        
        <div class="pipeline-detail">
          <h3>Reagent Entity Recognition</h3>
          <p>Specialized extraction of reagent information from protocols:</p>
          <ul>
            <li>Named entity recognition for chemicals and reagents</li>
            <li>Concentration and amount extraction</li>
            <li>Cross-protocol reagent linking</li>
            <li>Integration with lab inventory system</li>
          </ul>
          <p>This will enable precise reagent lookup and inventory integration.</p>
        </div>
      </div>
      
      <div class="column">
        <div class="pipeline-detail">
          <h3>Hierarchical Document Representation</h3>
          <p>Enhanced document structure preservation:</p>
          <ul>
            <li>Parent-child relationships between chunks</li>
            <li>Document tree visualization</li>
            <li>Contextual expansion during retrieval</li>
            <li>Section-aware query routing</li>
          </ul>
          <p>This will improve navigation within large documents like theses.</p>
        </div>
        
        <div class="pipeline-detail">
          <h3>Incremental Update System</h3>
          <p>Efficient handling of document updates:</p>
          <ul>
            <li>Differential document comparison</li>
            <li>Selective chunk reprocessing</li>
            <li>Version history maintenance</li>
            <li>Change notification system</li>
          </ul>
          <p>This will optimize processing for frequently updated protocols.</p>
        </div>
      </div>
    </div>
  </div>
</body>
</html>